[[Integration]]
= 数据集成
//文档整体配置
include::99_documentSetting.adoc[]

[NOTE]
DTinsight的基于Flink实现的数据集成底层模块现已开源，欢迎参与到开源项目的建设中。开源组件的详细使用方式请参考：
https://github.com/DTStack/flinkx[FlinkX数据集成组件]

数据集成模块是在各个存储单元之间执行数据交换的**通道**。为了在DTinsightIDE进行大规模数据集的挖掘与计算，通常的做法是在任务执行前将数据传输至DTinsightIDE，并在任务执行结束后将计算结果传输至外部存储单元（例如MySQL等应用数据库）。数据集成的作用如下图所示：

.数据输入与输出图
image::images\04_DTinsightIDEHelp_UserGuide_DataIntegration-57a3c.png[]

数据集成模块主要由以下几部分构成：

* 数据源管理
* 数据同步任务的配置与执行

[[integration_datasource]]
== 支持的数据源

不同的DTinsightIDE版本对数据源的支持能力不同，本版本支持的数据源包括：MySQL、SQLServer、Oracle、HDFS、Hive、HBase

[[integration_dataSourceManagement]]
== 数据源管理

**数据源管理是对外部存储单元访问参数的管理**，数据集成模块需要与<<deve_batch>>配合起来才能发挥作用，实际是由定时任务来执行数据传输的。

在项目上方的**数据集成**菜单，进入数据数据源管理页面，可看到目前已经集成的数据源列表，包括数据源名称、类型、连接信息、描述、最近修改人、最近修改时间、状态等信息，同时可执行编辑、删除等操作。

[[integration_dataSourceManagement_sourceConfig]]
=== 数据源的配置

** 在项目上方的**数据集成**菜单，进入项目源列表页面；
** 在**数据源列表**右上角的**新增数据源**，选择不同的数据源类型，需要填写不同的配置信息。

[[integration_dataSourceManagement_sourceConfig_RelationalDB]]
==== 关系型数据库（MySQL、Oracle、SQLServer）

**JDBC URL**:访问数据库的连接地址，格式如：

[source, java]
jdbc:mysql://10.203.8.76/tcipcccdrdb?useUnicode=true&characterEncoding=utf8

* **用户名**：访问数据库的用户名
* **密码**：访问数据库的密码
* **指定字符集**：通过`useUnicode=true&characterEncoding=utf8`来只是

[[integration_dataSourceManagement_sourceConfig_Hive]]
==== Hive

**JDBC URL**:访问Hive的连接地址，JDBC URL格式如：

[source, java]
jdbc:hive2://host:port/dbName

* **host**：Hive的host名或ip地址
* **port**：Hive的访问端口
* **dbName**：Hive的数据库名，默认情况下，可填写DTinsight·IDE默认的数据库名，此数据库名与当前项目的名称相同，您需要点击：**项目管理-项目配置**，数据库名即列表中的第一行**项目名称**，注意这里不是项目显示名称。如果您知晓此数据库名可直接填写。
* **用户名**：访问数据库的用户名
* **密码**：访问数据库的密码
* **DefaultFS**：默认情况下，此参数配置为：`hdfs://ns1`，如果您在Hadoop配置文件中修改了此配置，可以在`*/hadoop/etc/hadoop`路径下的`hdfs-site.xml`文件中，找到`dfs.nameservices`参数对应的值。
如下图所示：

image::images\04_DTinsightIDEHelp_UserGuide_DataIntegration-f6415.png[]

在此配置文件中，找到`dfs.nameservices`参数对应的值：

image::images\04_DTinsightIDEHelp_UserGuide_DataIntegration-22f56.png[]


* **高可用配置**：补充高可用配置参数，可以使DTinsightIDE访问高可用模式下的Hive数据源，高可用配置的示例如下：
[source, java]
{
  "dfs.nameservices": "nameserviceTest"
  "dfs.ha.namenodes.nameservice.nameserviceTest":"testDfs",
  "dfs.namenode.rpc-address.nameserviceTest.testDfs":"", "dfs.namenode.rpc-address.nameserviceTest.testDfs":"", "dfs.client.failover.proxy.provider.nameserviceTest": "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"
}

*** 关于HDFS高可用的更多信息可参考
http://hadoop.apache.org/docs/r2.7.4/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html[Hadoop官方文档]

[[integration_dataSourceManagement_sourceConfig_HDFS]]
==== HDFS
*** **DefaultFS**：即HDFS的namenode的节点地址，格式如：`hdfs://ServerIP:Port`。
*** **高可用配置**：补充高可用配置参数，可以使DTinsightIDE访问高可用模式下的HDFS数据源，高可用配置的示例如下：
[source, java]
"hadoopConfig":{
    "dfs.nameservices": "testDfs",
    "dfs.ha.namenodes.testDfs": "namenode1,namenode2",
    "dfs.namenode.rpc-address.testDfs.namenode1": "",
    "dfs.namenode.rpc-address.testDfs.namenode2": "",
    "dfs.client.failover.proxy.provider.testDfs": "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"
}
*** 关于HDFS高可用的更多信息可参考
http://hadoop.apache.org/docs/r2.7.4/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html[Hadoop官方文档]


[[integration_dataSourceManagement_sourceConfig_HBase]]
==== HBase

*** **Zookeeper集群地址**：
.. 必填，多个地址间用逗号分割。例如：`IP1:Port, IP2:Port, IP3:Port/子目录`。默认是`localhost`,在伪分布式模式时使用。若在完全分布式的情况下使用则需要修改。如果在hbase-env.sh设置了`HBASEMANAGESZK`， 这些ZooKeeper节点就会和HBase一起启动。
.. Port: ZooKeeper的zoo.conf中的配置。客户端连接的端口， 默认2181。
.. 子目录：HBase在ZooKeeper中配置的子目录。
*** **其他参数**：以JSON方式传入其他参数，例如： +
[source, java]
"hbaseConfig": {
  "hbase.rootdir": "hdfs: //ip:9000/hbase",
  "hbase.cluster.distributed": "true",
  "hbase.zookeeper.quorum": "***"
}


[[integration_dataSourceManagement_sourceConfig_connectionTest]]
==== 测试连通性

*** 在添加/编辑数据源时，可以主动测试数据源的连通性，只有数据源连接正常的情况下才可以被添加/编辑。

* 数据源的状态

已配置的数据源，系统会检测其是否在数据同步任务中被使用，“使用中”状态的数据源只能被编辑，但不能被删除

WARNING: 为了使DTinsightIDE可以访问对应的存储单元（包括读/写），请务必保证DTinsightIDE与这些存储单元之间的网络畅通，否则会导致任务失败。

WARNING: 若您的存储单元配置参数有变更，请及时修改数据源的配置，否则可能导致任务执行失败。

[[integration_jobConfig]]
== 数据同步任务的配置

数据同步任务的配置共分为5个步骤：

. 选择数据来源：选择已配置的数据源，系统会读取其中的数据；
. 选择数据目标：选择已配置的数据源，系统会向其写入数据；
. 字段映射：配置数据来源与数据目标之间的字段映射关系，不同的数据类型在这步有不同的配置方法；
. 通道控制：控制数据同步的执行速度、错误数据的处理方式等；
. 预览保存：再次确认已配置的规则并保存；

[[integration_jobConfig_relationalDB]]
=== 关系型数据库
DTinsightIDE支持MySQL、Oracle、SQLServer3种关系型数据库的数据同步，支持从从这几类数据库中读取/写入数据。

[[integration_jobConfig_relationalDB_read]]
==== 作为数据源

关系型数据库作为数据源，需配置以下信息：

* 选择数据源；
* 选择表；
* 数据过滤条件：针对源头数据筛选条件，根据指定的column、table、where条件拼接SQL进行数据抽取，暂时不支持limit关键字过滤。利用where条件可进行增量同步，具体说明如下：
+
增量导入在实际业务场景中，往往会选择当天的数据进行同步，通常需要编写where条件语句，请先确认表中描述增量字段（时间戳）为哪一个。如tableA增量的字段为create_time，则填写`create_time>您需要的日期`，如果需要日期动态变化，可以填写`${bdp.system.bizdate}`、`${bdp.system.cyctime}`调度参数，关于调度参数的详细信息请参考<<deve_batch_schedule_param_sys>>。
* 切分键：DTinsightIDE在进行数据抽取时，如果指定切分键，系统可以使用切分键的字段进行数据分片，数据同步因此会启动并发任务进行数据同步，这样可以大大提供数据同步的效能。
- 推荐将表的主键作为切分键，因为表主键通常情况下比较均匀，因此切分出来的分片也不容易出现数据热点。
- 目前DTinsightIDE目前支持MySQL数据库数值型切分键，支持Oracle数据库数值型、字符串类型切分键，不支持字符串、浮点、日期等其他类型，也不支持其他数据库。如果指定了不支持的类型，则忽略切分键功能，使用单通道进行同步。
- 如果不填写切分键，数据同步视作使用单通道同步该表数据 。
- 切分键的配置需要与第四步的“通道控制”联合使用，下面是典型的应用场景：
+
假设MySQL的tableA表中，有一个INT类型的字段名为id的主键，那么可以在切分键输入框中输入id字段，并在第四步的“作业并发数”中配置为5，则系统会产生5个并发线程同时读取MySQL表，每个任务会根据id字段的值进行数据切分，保证5个并发线程读取的数据是不同的，通过以上机制即可以加速数据读取的速度。

* 映射配置：
- 支持挑选部分字段进行导出。
- 支持字段换序，即字段可以不按照表schema信息顺序进行导出。
//- 支持常量配置，用户需要按照MySQL SQL语法格式，比如 [“id”, “table“, “1”, “‘mingya.wmy’”, “‘null’”, “to_char(a + 1)”, “2.3” , “true”]。 id为普通列名，table为包含保留字的列名，1为整形数字常量，’mingya.wmy’ 为字符串常量（注意需要加上一对单引号），null为空指针，CHAR_LENGTH(s)为计算字符串长度函数，2.3 为浮点数，true为布尔值。

[[integration_jobConfig_relationalDB_write]]
==== 作为数据目标

关系型数据库作为数据目标，需配置以下信息：

* 选择数据同步目标；
* 选择表；
* 导入前、导入后准备语句：执行数据同步任务之前率先执行的SQL语句。目前向导模式只允许执行一条 SQL 语句，例如：清除旧数据。
* 主键冲突的处理：
- replace into模式：**目前只有MySQL数据库支持replace into模式** 没有遇到主键/唯一性索引冲突时，与insert into行为一致，冲突时会用新行替换原有行所有字段；
- insert into模式：当主键/唯一性索引冲突时会写不进去冲突的行，以脏数据的形式体现，脏数据的配置与管理请参考<<datamang_dirtydata>>；
* 映射配置：支持挑选部分字段进行写入。

[[integration_jobConfig_hive]]
=== Hive

[[integration_jobConfig_hive_read]]
==== 作为数据源

* 选择数据源；
* 选择表；
* 分区：读取数据所在的分区信息，支持直接填写分区名称，同时支持系统调度参数:
[source, SQL ]
${bdp.system.premonth}
${bdp.system.cyctime}
${bdp.system.bizdate}
${bdp.system.currmonth}


[[integration_jobConfig_hive_write]]
==== 作为数据目标

* 选择数据源；
* 选择表；
* 分区：需要写入数据表的分区信息，必须指定到最后一级分区。例如把数据写入一个2级分区表，可以通过`pt=20180401/ds=abcdef`。其中pt是一级分区字段，ds是二级分区字段。
- 对于非分区表，该值务必不要填写，表示直接导入到目标表。
- DTinsightIDE不支持数据路由写入，对于分区表请务必保证写入数据到最末一级分区。
- 分区的填写支持直接填写分区名称，同时支持系统调度参数:
[source, SQL ]
${bdp.system.premonth}
${bdp.system.cyctime}
${bdp.system.bizdate}
${bdp.system.currmonth}

* 写入模式：
insert overwrite：写入前将清理已存在的数据文件，之后执行写入；
insert into：写入前已有的数据不会删除，系统会写入的新的文件；

[[integration_jobConfig_hdfs]]
=== HDFS

[[integration_jobConfig_hdfs_read]]
==== 作为数据源

HDFS作为数据源，需配置以下信息：

* 选择数据源；
* 读取的HDFS文件路径：要读取的文件路径，多个路径可以用逗号隔开；
* 读取的HDFS文件类型：目前只支持用户配置为"text"、"orc" ，text表示textfile文件格式，orc表示orcfile文件格式；
* 编码格式：读取文件的编码配置，默认为UTF-8，还支持GBK；
* 分隔符：读取的字段分隔符。需要注意的是，读取textfile数据时，需要指定字段分割符（默认值\001），但在读取orcfile时，用户无需指定字段分割符；
* 映射配置：
- 支持挑选部分字段进行导出。
- 支持字段换序，即字段可以不按照表schema信息顺序进行导出。
//- 支持常量配置，用户需要按照MySQL SQL语法格式，比如 [“id”, “table“, “1”, “‘mingya.wmy’”, “‘null’”, “to_char(a + 1)”, “2.3” , “true”]。 id为普通列名，table为包含保留字的列名，1为整形数字常量，’mingya.wmy’ 为字符串常量（注意需要加上一对单引号），null为空指针，CHAR_LENGTH(s)为计算字符串长度函数，2.3 为浮点数，true为布尔值。

[[integration_jobConfig_hdfs_write]]
==== 作为数据目标

* 选择数据源；
* 输入HDFS文件路径：存储到Hadoop hdfs文件系统的路径信息，HdfsWriter会根据并发配置在此目录下写入多个文件。为与hive表关联，请填写hive表在hdfs上的存储路径。例：Hive上设置的数据仓库的存储路径为：/user/hive/warehouse/ ，已建立数据库：test，表：hello；则对应的存储路径为：/user/hive/warehouse/test.db/hello；
* 分隔符：写入时的字段分隔符,需要用户保证与创建的Hive表的字段分隔符一致，否则无法在Hive表中查到数据；
* 编码格式：默认为utf-8，请慎重修改；
* 文件名：写入时的文件名；
* 输入HDFS文件类型：目前只支持用户配置为"text"、"orc" ，text表示textfile文件格式，orc表示orcfile文件格式；
* 写入模式：写入前数据清理处理模式：append，追加；overwrite，覆盖；
* 映射配置：
- 支持挑选部分字段进行写入。
- 支持字段换序，即字段可以不按照表schema信息顺序进行写入。

[[integration_jobConfig_hbase]]
=== HBase

[[integration_jobConfig_hbase_read]]
==== 作为数据源

[[integration_jobConfig_hbase_write]]
==== 作为数据目标


[[integration_jobConfig_channel]]
=== 通道控制

[[integration_jobConfig_channel_speed]]
==== 同步速率与作业并发

* **作业速率上限**

设置作业速率上限，则数据同步作业的总速率将尽可能按照这个上限进行同步，此参数需根据实际硬件配置调整，默认为1，可支持1-20MB/s的同步速度。

当数据量较大，且硬件配置较好时，可以提高作业速率上限，DTinsightIDE将会提高同步速度，使用较短的时间完成同步。

* **作业并发数**

作业速率上限=作业并发数*单作业的传输速率，当作业速率上限已定，选择的并发数越高则单并发的速率越低，同时所占用的内存会越高，这可以根据你的业务需求选择设定的值。

作业并发数下拉框选择对象，取决作业速率上限。作业速率上限选择值nMB/s，作业并发数最大能选择n

[NOTE]
流量度量值是数据集成本身的度量值，不代表实际网卡流量，实际流量膨胀看具体的数据存储系统传输序列化情况。

[NOTE]
关系型数据库设置作业速率上限和切分键才能根据作业速率上限将表进行切分，关系型数据库只支持数值型作为切分键，但Oracle数据库支持数值型和字符串类型作为切分键。

[[integration_jobConfig_channel_dirty]]
==== 错误数据处理

* **错误记录管理**

在数据同步任务中，可开启错误记录管理，开启后，任务会将数据同步过程中的错误数据写入指定的Hive表（可指定表名、生命周期）。

在数据同步任务的周期执行过程中，每个实例都会将脏数据写入一个分区，每个实例一个分区。

* **脏数据数量控制**

支持对于脏数据的自定义监控和告警，包括对脏数据最大记录数阈值，当传输过程出现的脏数据大于指定的数量，则报错退出。

支持对于脏数据占比的统计，当任务执行结束后系统统计脏数据占总的同步数据的占比，超过阈值时将任务置为失败。

[NOTE]
错误记录占比是数据同步实际执行结束后统计的，并不会在任务同步过程中统计。

[[integration_jobConfig_wholeDatabase]]
=== 整库迁移

[[integration_jobConfig_wholeDatabase_overview]]
==== 整库迁移概述

整库迁移是帮助提升用户效率、降低用户使用成本的一种快捷工具，它可以快速把一个MySQL数据库内所有表一并上传到DTinsightIDE（Hive），节省大量初始化数据上传的批量任务创建时间。

假设数据库有100张表，您原本可能需要配置100次数据同步任务，但有了整库上传便可以一次性完成。同时，由于数据库的表设计规范性的问题，此工具并无法保证一定可以一次性完成所有表按照业务需求进行同步的工作，本模块具有一定的约束性，本节将主要从功能性和约束性对整库上传进行介绍。

[[integration_jobConfig_wholeDatabase_overview_create]]
===== 任务生成规则

完成配置后，根据选择的需要同步的表，依次创建Hive表，生成数据同步任务。

Hive表的表名、字段名和字段类型根据高级配置生成，如果没有填写高级配置，则与MySQL表的结构完全相同。表的分区为pt，格式为yyyymmdd。

生成的数据同步任务是按天调度的周期任务，会在第二天凌晨自动运行，传输速率为1M/s，它在细节上会因为同步的方式、并发配置等有所不同，您可以在同步任务目录树的**clone_database>数据源名称>mysql2hive_表名**中找到生成的任务，然后对其进行更加个性化的编辑操作。


[[integration_jobConfig_wholeDatabase_overview_constraint]]
===== 约束限制

由于数据库的表设计规范性的问题，整库上传具有一定的约束性，具体如下：

* **目前仅提供MySQL数据源的整库上传到Hive**
* **仅提供每日增量、每日全量的上传方式**
* 如果您需要一次性同步历史数据，则此功能无法满足您的需求，故给出以下建议：
** 建议您配置为每日任务，而非一次性同步历史数据。您可以通过调度提供的补数据来对历史数据进行追溯，这样可避免全量同步历史数据后，还需要做临时的SQL任务来拆分数据。
** 如果您需要一次性同步历史数据，可以在任务开发页面进行任务的配置，然后单击**运行**，完成后通过SQL语句进行数据的转换，因为这两个操作均为一次性行为。
** 如果您每日增量上传有特殊业务逻辑，而非一个单纯的日期字段可以标识增量，则此功能无法满足您的需求，为了更方便地增量上传，建议您在创建所有数据库表的时候都有：gmt_create, gmt_modify字段，同时为了效率更高，建议增加id为主键。
* **整库上传提供分批和整批上传的方式**
** 为了保障对数据库的压力负载，整库上传提供了分批上传的方式，您可以按照时间间隔把表拆分为几批运行，避免对数据库的负载过大，影响正常的业务能力。以下有两点建议：
*** 如果您有主、备库，建议同步任务全部同步备库数据。
*** 批量任务中每张表都会有1个数据库连接，上限速度为1M/s。如果您同时运行100张表的同步任务，就会有100个数据库进行连接，建议您根据自己的业务情况谨慎选择并发数。
** 如果您对任务传输效率有自己特定的要求，此功能无法实现您的需求。所有生成任务的上限速度均为 1M/s。
* **仅提供整体的表名、字段名及字段类型映射**

整库上传会自动创建Hive表，分区字段为pt，类型为字符串string，格式为yyyymmdd。

[NOTE]
选择表时必须同步所有字段，不能对字段进行编辑。

[[integration_jobConfig_wholeDatabase_config]]
==== 配置整库迁移

整库迁移是为了提升用户效率、降低用户使用成本的一种快捷工具，它可以快速完成把MySQL数据库内所有表一并上传到DTinsightIDE（Hive）的工作，关于整库迁移的详细介绍请参见<<integration_jobConfig_wholeDatabase_overview>>。

本文将通过实践操作，为您介绍如何使用整库迁移功能，完成MySQL数据整库迁移到DTinsightIDE（Hive）。

[[integration_jobConfig_wholeDatabase_config_step]]
===== 操作步骤

. 登录到**DTinsightIDE>数据集成**，进入数据源管理页面。
. 单击右上角的新增数据源，添加一个面向整库迁移的MySQL数据源（这里假设为MySQL_Migrate），单击测试连通性验证数据源访问正确无误后，确认并保存此数据源。
. 新增数据源成功后，即可在数据源列表中看到新增的MySQL数据源MySQL_Migrate。单击**整库迁移**，即可进入对应数据源的整库迁移功能界面，整库迁移界面主要分为3块功能区域：
.. 待迁移表筛选区，此处将MySQL数据源MySQL_Migrate下的所有数据库表以表格的形式展现出来，您可以根据实际需要批量选择待迁移的数据库表。
.. 高级设置，此处提供了MySQL数据表和DTinsightIDE数据表的表名称、列名称、列类型的映射转换规则。
.. 迁移模式、并发控制区，此处可以控制整库迁移的模式（全量、增量）、并发度配置（分批上次、整批上传）、提交迁移任务进度状态信息等。
.. 单击高级设置按钮，您可以根据您具体需求选择转换规则。比如DTinsightIDE端建表时统一增加了**ods_**这一前缀。
.. 在迁移模式、并发控制区中，选择同步方式为每日增量，并配置增量字段为gmt_modified，数据集成默认会根据您选择的增量字段生成具体每个任务的增量抽取where条件，并配合DTinsightIDE调度参数（例如：${bdp.system.bizdate}）形成针对每天的数据抽取条件。
.. 数据集成抽取MySQL库表的数据是通过JDBC连接远程MySQL数据库，并执行相应的SQL语句，将数据从MySQL库中select出来。由于是标准的SQL抽取语句，可以配置where子句控制数据范围。
..为了对源头MySQL数据源进行保护，避免同一时间点启动大量数据同步作业带来数据库压力过大，可选择分批上传模式，并配置从每日0点开始，每1小时启动3个数据库表同步。
. 最后，单击提交任务按钮，这里可以看到迁移进度信息，以及每一个表的迁移任务状态。
. 单击对应的迁移任务，会跳转到数据集成的任务开发界面，在左侧目录树clone_database目录下，会有对应的所有整库迁移任务，任务命名规则是：mysql2hive_源表名。

此时便完成了将一个MySQL数据源MySQL_Migrate整库迁移到DTinsight的工作。这些任务会根据配置的调度周期（默认天调度）被调度执行，您也可以使用DTinsight调度补数据功能完成历史数据的传输。通过整库迁移功能可以极大减少您初始化的配置、迁移成本。

[[integration_jobConfig_version]]
=== 版本支持

DTinsightIDE数据集成模块对不同数据库的版本支持情况如下表所示：

[cols="1, 5", options="header"]
|===
|数据源类型
|支持的版本
|MySQL
|5.0及以上
|Oracle
|11g及以上
|SQLServer
|SQLServer 2008及以上
|Hive
|
|HDFS
|
|ElasticSearch
|
|HBase
|
|===

[[integration_demo]]
== 最佳实践

[[integration_demo_mysqlhive]]
=== MySQL导入Hive

[[integration_demo_hivemysql]]
=== Hive导入MySQL

[[integration_demo_mysqlincre]]
=== MySQL数据库增量同步

经<<integration_dataSourceManagement>>的数据源配置后，DTinsightIDE即可以从已配置的数据源中读取或写入数据。下面以MySQL->HDFS，并由DTinsightIDE访问HDFS中的数据为例，介绍操作流程：

[[integration_demo_mysqlhdfs]]
=== MySQL导入HDFS

进入<<deve_batch_develop>>模块，进行如下配置：

.. **数据来源：** 类型选择MySQL，并选中刚刚配置的数据源`dataSourceExample`，选择其中的表`tableAge`，数据过滤规则、切分键不填写。

.. **选择目标：** 类型选择HDFS，并选中刚刚配置的数据源`hdfsSourceExample`，其他配置参数为：

... **路径：**需要填写的格式为`user/hive/warehouse/项目名称.db`。
... **分隔符：**用户自定义，或者不填写，不填写时，会采用HIVE默认的分隔符`\001`。
... **编码：**可选择UTF-8或GBK。
... **文件名：**这里实际上填写的是hive中的表名（本例子中填写为`tableNameAge`）。
... **文件类型：**支持TEXT和ORC类型。
... **主键冲突规则：**即执行数据同步时，若出现主键冲突，则报错或忽略。

.. **字段映射：** 通过映射的方式，将左右2侧的字段映射起来，用户可通过同名映射或同行映射快速配置。对于HDFS文件，可能出现无法读取字段的问题，用户需手动配置每个HDFS字段。

.. **配置调度周期与依赖关系：**在右侧的任务面板中配置任务的生效日期、调度日期、起调时间、需依赖的上游任务、自依赖等信息，详细配置方式请参考<<deve_batch_schedule>>。

.. **发布任务：**确认任务信息无误后，提交执行，发布成功后，此任务不会立即执行，而是会在当天（或第二天）的22:00生成任务实例，待执行条件满足后才执行，如果需要数据同步任务立即执行，请参考<<deve_batch_schedule>>中的**补数据**或<<QA>>。

=== 执行数据同步任务

任务提交成功后，为了立即执行，需在<<maint_batch_jobmanage>>中设置**补数据**任务，即可立即进行数据同步。

=== 新建hive表，并读取数据

在<<maint_batch_jobmanage>>模块中进行如下操作：

. 点击**新建表**，建立**内部表**或**外部表**，内部表与外部表的区别请参考<<QA>>。或点击**DDL建表**，通过SQL的形式建表。
. 新建表的表名，必须与<<syncJobConfig_mysql-hdfs>>中的**文件名**对应，否则会导致无法关联到对应的文件，造成表中没有数据。
. 新建表的编码格式，路径等信息，也必须与<<syncJobConfig_mysql-hdfs>>中的**编码格式**、**路径**对应起来，否则会造成无法关联到对应的文件，造成表中没有数据或编码错误。
. 新建表的字段信息，必须与数据同步任务中的字段对应，否则会造成无法关联到对应的文件。
. 建表成功后，即实现当前表与HDFS数据之间的关联，即可以在<<jobDevelopment>>中当前项目的任意一个SQL查询到数据。

[NOTE]
.新建hive表的注意事项
<<createTableAndReadData>>与<<runSyncJob>>两个步骤不区分先后顺序，但数据同步任务时填写的**路径**、**文件名**、**编码**等参数需要与建hive表时的**路径**、**表名**、**编码**对应起来

NOTE: 在本例中，主键冲突规则的配置生效，是依赖于MySQL中`tableAge`的主键，
