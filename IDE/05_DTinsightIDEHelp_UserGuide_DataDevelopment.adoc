[[DataDevelopment]]
= 数据开发
//文档整体配置
include::99_documentSetting.adoc[]

数据开发页面包括实时任务开发与离线任务开发，任务开发模块是根据业务需求，设计数据计算流程并编码的模块。下面分为实时任务开发与离线任务开发2个模块分别介绍。


[[deve_batch]]
== 离线任务开发

离线任务开发模块主要是设计数据计算流程，并实现为多个相互依赖的任务，供调度系统自动执行的主要操作页面。

**对象**

在数据开发阶段，DTinsightIDE提供了4种对象：任务、脚本、资源和函数。它们之间的项目关系如下图所示：

image::images\05_DTinsightIDEHelp_UserGuide_DataDevelopment-adee5.png[]

每个对象的说明如下：

* 任务：数据开发的主要对象，包含周期属性和依赖关系，是数据计算的主要载体，支持多种类型的任务和节点适应不同场景，详情请参见<<deve_batch_type>>。
* 脚本：数据开发的辅助对象，不包含周期属性和依赖关系，主要用于实现非周期的临时数据处理，如临时表的增删改等，详情请参见<<deve_batch_script>>。
* 函数和资源：任务中的代码运行时需要引用的一些文件和计算函数，在任务正式执行前需要上传，详情请参见<<deve_batch_resource>>和<<deve_batch_function>>。


**流程**

一个任务的开发和使用流程如下图所示：

image::images\05_DTinsightIDEHelp_UserGuide_DataDevelopment-15f79.png[]

**任务运行说明**

由流程的介绍可知，DTinsightIDE提供了3种运行方式，以使任务中的计算语句生效，适用场景和限制条件如下：

[cols="1, 1, 2, 2, 3, 3", options="header"]
|===
|操作
|触发方式
|运维中心是否有实例生成
|调度属性情况
|适用场景
|特殊说明
|页面直接运行
|手动触发
|否
|不受调度周期和依赖关系影响
|适用于代码调试阶段，无需保存提交
|支持脚本和任务，但任务、脚本类型仅支持SQL1种
|系统自动运行
|系统触发
|是
|受调度周期和依赖关系影响
|是使用DTinsightIDE实现数据自动计算的主要方式，需要运维人员在运维中心维护所有周期实例按序成功执行
|仅支持任务，不支持脚本，且使用最新提交的版本
|补数据运行
|手动触发
|是
|受调度周期和依赖关系影响
|是对系统自动运行方式的补充，部分任务由于新建或者出错，需要触发今天之前一段时间的数据计算时使用该功能
|仅支持任务，且使用最新提交的版本
|===

[[deve_batch_develop]]
===	任务开发

任务开发的过程实际上就是对表的操作过程，本节将从创建任务、创建表开始。

[[deve_batch_develop_create]]
==== 新建任务

DTinsightIDE提供了5种任务类型，本节以创建SQL任务为例，介绍如何创建一个任务并编辑代码内容。更多任务类型的使用请参见<<deve_batch_type>>。

* 新建SQL任务

进入“数据开发”菜单，点击“新建离线任务”按钮，并填写新建任务弹出框中的配置项，配置项说明：

. 任务名称：需输入英文字母、数字、下划线组成，不超过64个字符。
. 任务类型：可选择SQL、MR、数据同步、Python、虚节点。
. 存储位置：此任务在页面左侧的任务存储结构中的位置。
. 描述：此任务的描述，可输入长度不超过200个的任意字符。

点击“保存”，弹窗关闭，即完成了新建任务，同时系统自动打开新建的SQL任务。

* 编辑SQL任务代码

SQL任务创建好后，可以在代码编辑器中编写SQL语句（该SQL的语法为Hive SQL，与传统关系型数据库的SQL语法有所不同，详细的SQL编辑说明请
https://cwiki.apache.org/confluence/display/Hive/LanguageManual[Hive SQL的编码说明]）。

编写的SQL语句示例如下：

[source, SQL]
select * from bank_data;

查询结果最多只展示1000条数据，DTinsightIDE不支持在页面上下载大量数据。

如果执行的是多个SQL语句，会根据顺序依次下发执行。

* 配置节点任务的调度属性

DTinsightIDE提供了丰富的时间周期和依赖关系支持，并提供了基于时间的系统参数和自定义参数支持，请参考相应文档选择适合您业务需要的配置方式。

代码和参数配置调试完毕后，一个周期任务需要发布以后才会触发调度系统按配置周期定时产生运行实例并执行代码，提交任务的具体操作请参见<<deve_batch_develop_release>>。

为使周期任务运行并在每次运行时适应上下文环境，需要配置时间周期和参数。

[NOTE]
由于节点任务有周期调度属性，因此内容建议以计算类语句为主，表操作语句建议使用可视化建表和脚本开发等其他功能来运行和维护。

[[deve_batch_develop_table]]
==== 创建表

创建表有2种方式可视化建表和SQL建表。

* 可视化建表

进入某个项目后，点击**数据管理>表管理**，单击右上角的新建表，按照可视化建表页面进行操作，即可成功建表。

* SQL建表

如果习惯编辑SQL语句，或希望一次运行多个建表语句，或需要修改表结构等，可通过SQL脚本进行脚本开发，详情请参见<<deve_batch_script>>。

新建一个类型为SQL的脚本文件，在编辑区填写任意SQL语句（包括新建或修改表的DDL语句）并单击直接运行。

[NOTE]
DTinsightIDE的建表语句与通用的Hive不同，建表时需要指定生命周期

下面是建表SQL代码的举例
[source, SQL]
CREATE TABLE big_table2 (a string, b string) PARTITIONED BY (ds string) lifecycle 10;

建表语法为：
[source, SQL]
CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment]
[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]
[CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]
[ROW FORMAT row_format]
[STORED AS file_format]
[LOCATION hdfs_path]
lifecycle N;

其中lifecycle后面的N表示此表的生命周期有多少天。


* 查看表
+
查看表的方式有两种：表查询和查找数据，详情如下：
+
** 表查询：建表成功后，单击数据开发页面左侧导航栏的表查询，找到新建的表进行查询，详情请参见<<deve_batch_tableview>>。
** 查找数据：建表成功后，点击**数据管理>表管理**，搜索表名，找到新建的表进行查询。

[[deve_batch_develop_run]]
==== 运行任务

DTinsightIDE目前支持3种方式来使一个任务中的代码对数据生效：页面直接运行，系统自动周期运行和补数据运行。页面直接运行适用于代码调试修改，不考虑调度属性配置的情况，或者是不需要提交的直接运行的对象如<<deve_batch_script>>等。本文将以SQL节点任务为例，说明如何在代码编辑页面直接运行。

[NOTE]
仅SQL任务、脚本2种类型支持页面直接运行，其他类型不支持页面直接运行。

* 页面直接运行一个任务

单击一个SQL任务打开编辑区，选择想要执行的部分语句，然后在操作区单击**运行**按钮即可触发选定代码执行。如果不选择部分代码，而是直接单击**运行**，则会默认运行当前任务的全部代码。

* 查看运行日志和结果

任务触发运行后，在编辑区下方会显示日志页，如果有语句的运行结果返回了数据集，则在日志页旁显示结果页，也支持结果下载。

无论运行几次，日志页只有一个，仅显示最近一次触发运行的日志信息，之前的日志会被覆盖。结果页可以存在多个，按语句执行顺序依次显示，最多可以显示20个结果页，方便您进行对比数据等操作。

多个语句触发执行时，这些语句将串行执行，日志内容依次显示在日志页中，结果则按每个语句的执行顺序分别显示在不同的结果页中。

[NOTE]
在页面上点击“运行”时，运行select语句时，页面最多可显示数据集的前1000条记录，故请控制每次查询产生的记录数。


[[deve_batch_develop_release]]
==== 发布任务

提交任务操作，使得一个周期任务的代码和周期配置进入调度系统，从第二天开始，调度系统将根据该任务的周期配置每天生成实例并定时运行，直到该任务被删除，调度系统才会停止为该任务生成实例并运行。

[NOTE]
新增或修改任务时，如果当天22点00前提交成功，则在第二天的实例中即可看到结果；如果当天22点00后提交成功，则在第三天的实例中才会看到结果。

[NOTE]
一个周期任务只有提交成功后才会进入调度系统，从而使得调度系统按配置周期定时产生实例并运行。

* 发布SQL任务

单击打开该任务，在右上角点击“发布”按钮，可输入发布的备注信息，点击“确定”。

* 查看任务历史版本

发布任务之后，系统会产生一条版本记录，通过点击**任务开发-任务属性-历史发布版本**中查看发布的时间和备注信息的。

* 对比任务版本

在SQL任务的**历史发布版本**中，可以查看到曾经提交过的代码。选择查看提某个代码版本，点击**代码**，系统可对比当前代码与选中版本的代码的差别。

[[deve_batch_develop_frozen]]
==== 冻结任务

如果需要让某个任务停止运行一段时间，可以在**任务开发**模块打开某任务，在右侧**调度依赖**面板中勾选**冻结**，表示此任务进入冻结状态

* 处于冻结状态的任务，其周期实例依然会生成，但不会运行。
* 对于存在依赖关系的多个任务，如果将上游任务A冻结，则下游任务B也会进入“冻结”状态，B任务的实例也会产生，但不会运行，在B任务的执行日志中会打印出是由于A任务被冻结才没有运行的。
* 周期任务的冻结，是第二天生效的，且冻结状态的任务，生成的实例也是冻结状态，不会直接运行，必须将实例解冻后再单击重跑，才会运行；如果需要紧急冻结任务，可以在周期实例中进行冻结操作。

[NOTE]
依然可以对冻结状态的任务执行补数据，补数据实例会正常运行

[[deve_batch_develop_delete]]
==== 删除任务

如果在编辑过程中想要放弃一个任务编辑版本，或者周期任务提交后想从调度系统中去掉该任务的自动运行，可以在左侧的任务面板中右键点击此任务，选择**删除**。

* 如果此任务被其他任务依赖（是其他任务的上游任务），则此任务不能被删除，您需要先解除依赖关系再进行删除
* 任务删除后，已生成的任务实例不会被删除，但会运行失败

[[deve_batch_develop_search]]
==== 搜索任务

当任务数量很多时，可以在左侧的**任务管理**面板中点击**搜索任务**，或按下Ctrl+P快捷键，通过输入任务名称并按下回车搜索并打开任务。


[[deve_batch_type]]
===	任务类型

DTinsightIDE提供了5种任务类型，分别适用于不同的使用场景：

* SQL任务

SQL任务支持您直接在Web端编辑和维护SQL代码，并可方便地调试运行和协作开发。DTinsightIDE还支持代码内容的版本管理和上下游依赖自动解析等功能，使用示例请参见<<deve_stream_develop_create>>。

DTinsightIDE的SQL任务的代码内容遵循Hive的语法。Hive SQL的语法请参考
https://cwiki.apache.org/confluence/display/Hive/LanguageManual[Hive官方文档]

* MR任务

MR任务用于在Spark的MapReduce编程接口（Java API）基础上实现的数据处理程序的周期运行，详细的编码规则请参考
https://spark.apache.org/docs/latest/api/java/index.html[Spark Java API官方文档]。

DTinsightIDE完全按照Spark官方的编程接口，您可以将代码打包成为JAR类型的资源文件上传到DTinsightIDE中，然后配置MR任务。

* 数据同步任务

数据同步任务主要完成数据在不同存储单元之间的迁移，详细的数据同步任务配置规范请参考<<Integration>>模块。

* Python任务

Python任务用于在Spark的Python编程接口（Python API）基础上实现的数据处理程序的周期运行，详细的编码规则请参考
https://spark.apache.org/docs/latest/api/python/index.html[Spark Python API官方文档]。

DTinsightIDE完全按照Spark官方的编程接口，您可以将代码打包，并以资源文件的形式上传到DTinsightIDE中，然后配置Python任务。

* 虚节点任务

虚拟节点属于控制类型节点，它不产生任何数据的空跑节点，常用于多个任务统筹节点的根节点。

[NOTE]
多个任务中最终输出表有多个分支输入表，且这些输入表没有依赖关系时便经常用到虚拟节点。

假设：输出表由3个数据同步任务导入的源表经过SQL任务加工产出，这3个数据同步任务没有依赖关系，SQL任务需要依赖3个同步任务，则任务依赖关系如下图所示：

image::images\05_DTinsightIDEHelp_UserGuide_DataDevelopment-920c1.png[]
用一个虚节点任务作为起始根节点，3个数据同步任务依赖虚节点，SQL加工任务依赖3个同步任务。

[NOTE]
虚节点任务没有日志信息：虚节点任务不会真正的执行，等到虚节点运行的时候，便会直接被置为成功，所以虚节点没有日志信息。

[[deve_batch_schedule]]
===	调度属性配置

[[deve_batch_schedule_cycle]]
==== 周期配置

任务的时间属性目前支持月、周、天、小时和分钟5种配置方式，本文将分别介绍配置方式和调度系统中的实例运行情况。

[NOTE]
一个周期运行的任务，它的依赖关系的优先级大于时间属性。在时间属性决定的某个时间点到达时，任务实例不会马上运行，而是先检查上游是否全部运行成功。

* 上游依赖的实例没有全部运行成功**并且**定时运行时间已到，则实例不会运行。
* 上游依赖的实例全部运行成功**并且**定时运行时间还未到，则实例不会运行。
* 上游依赖的实例全部运行成功**并且**定时运行时间已到，则实例具备了运行的条件，待其获得集群的计算资源后即可以开始运行，集群资源的分配由Spark集群分配，用户不可干预。

对于离线任务（包括SQL、MR、数据同步），可在“调度依赖”侧边栏，可配置此任务的调度周期等参数，具体包括如下内容：

* **调度属性** +
调度属性的配置包括如下内容：此任务接受调度还是停止调度？此任务在什么时间生效？多长时间间隔运行一次？具体什么时候运行？具体包括： +

. 调度状态 +
选中“冻结”，表示此任务停止调度，不会进行实际的计算（通常此功能用于暂时不需要运行，但也不想删除的任务）；
若勾选“冻结”，任务每天仍会产生实例，但调度时会直接返回失败状态，不会真正运行任务逻辑。
. 生效日期：任务只在生效日期内执行；
. 调度周期：分为{天；周；月；小时；分钟}，若选中“天”，则表示此任务每天执行一次；
. 起调时间：用户设定调度周期后，还需要设定具体在哪个时刻点启动任务。根据用户选择的调度周期不同，起调时间需要配置不同的参数：

[cols="1, 5", options="header"]
|===
|调度周期
|起调时间配置
|日
a|
天调度任务，即每天自动运行一次。新建周期任务时，默认的时间周期为每天0点运行一次，可根据需要自行指定运行时间点，配置“具体时间”：

* 小时：单选下拉列表，00-23，默认选中00
* 分钟：单选下拉列表，00-59，默认选中00

|周
a|
周调度任务，即每周的特定几天里每天在特定时间点自动运行一次，需配置“选择时间”和“具体时间”：

* 选择时间：复选下拉列表{星期一；星期二；……星期日}，可复选，默认选中“星期一”；
* 具体时间：同“调度周期-日”相同；

|月
a|
月调度任务，即每月指定的特定几天里每天在特定时间点自动运行一次，需配置“选择时间”和“具体时间”：

* 选择时间：复选下拉列表{每月最后一天；每月1号；每月2号；……每月31号}，可复选，默认选中“星期一”；
* 具体时间：同“调度周期-日”相同；

|小时
a|
小时调度任务，即每天指定的时间段内按N*1小时的时间间隔运行一次，比如每天1点到4点的时间段内，每1小时运行一次。当调度周期切换到非天级调度时，节点起调时间将不可选，需配置“开始时间”、“结束时间”和“间隔时间”：

* 开始时间、结束时间：同“调度周期-日”类似，小时可选择00-23，分钟不可选，开始时间的分钟为0，结束时间的分钟为59；
* 间隔时间：单选下拉列表{1小时；2小时……23小时}，默认选中1小时；
* 开始时间，应早于结束时间，若不符合，需提示用户“开始时间应早于结束时间”；

|分钟
a|
分钟调度任务，即每天指定的时间段内按N*指定分钟的时间间隔运行一次，目前能支持的最短时间间隔为每5分钟运行一次，当调度周期切换到非天级调度，节点起调时间将不可选，需配置“开始时间”、“结束时间”和“间隔时间”：

* 开始时间、结束时间：同“调度周期-小时”相同；
* 间隔时间：单选下拉列表{5分钟；10分钟；……55分钟}，默认选中5分钟；
* 开始时间，应早于结束时间，若不符合，需提示用户“开始时间应早于结束时间”；
|===

[NOTE]
按照小时周期调度时，时间周期按**左闭右闭**原则计算，比如配置为从0点到2:59点的时间段内，每隔1个小时运行一次，表明时间区间为[00:00，02:59]，间隔为1小时，调度系统将会每天生成3个实例，分别在0:00／1:00／2:00运行。

* **任务实例的生成**

DTinsightIDE在每天22:00统一生成第二天所有需要的任务实例，基于以上设计，任务开发时需要注意任务的提交时间，这里以一个天周期调度任务A为例：

image::images\05_DTinsightIDEHelp_UserGuide_DataDevelopment-4dc8e.png[]

任务A基本信息：调度周期：1天；具体调度时间：8:00； +
1）若您在1月1日21:00提交A任务（时间轴上侧），DTinsightIDE会在当天22:00产生A任务的实例，并会在1月2日8:00第一次运行； +
2）若您在1月1日23:00提交A任务（时间轴下侧），由于DTinsightIDE已经在22:00产生了1月2日的所有实例，A任务在1月2日将不会运行。A任务的实例将会在1月2日22:00产生，并在1月3日第一次运行。

[TIP]
**任务被配置为冻结是否生成实例？** +
处于冻结状态的任务，其周期实例依然会生成，但不会运行。 +
对于存在依赖关系的多个任务，如果将上游任务A冻结，则下游任务B也会进入“冻结”状态，B任务的实例也会产生，但不会运行，在B任务的执行日志中会打印出是由于A任务被冻结才没有运行的。


[TIP]
**任务被删除是否会影响实例的运行？** +
如果此任务被其他任务依赖（是其他任务的上游任务），则此任务不能被删除，您需要先解除依赖关系再进行删除 +
任务删除后，已生成的任务实例不会被删除，但会运行失败

[TIP]
**想在每月的最后一天计算当月数据怎么办？** +
目前系统不支持配置 每月最后一天，因此如果时间周期选择每月31日，那么在有31日的月份会有一天调度，其他日期都是生成实例然后直接设为运行成功 +
需要统计每个月的数据时，建议选择每月的1日运行，计算上个月的数据

[[deve_batch_schedule_param]]
====	参数配置

为使任务自动周期运行时能动态适配环境变化，DTinsightIDE提供了参数配置的功能。详细的参数配置如下表所示：

[cols="1,5,1,4", options="header"]
|===
|参数类型
|设置方式
|适用类型
|参数编辑框示例
|系统参数bdp.system.bizdate、bdp.system.cyctime、bdp.system.premonth、bdp.system.currmonth
|在调度系统中运行时，无须在编辑框设置，可直接在代码中引用${bdp.system.bizdate}、${bdp.system.cyctime}、${bdp.system.premonth}、${bdp.system.currmonth}， **系统将自动替换这些参数的取值**
|全部任务类型，不支持脚本
|无
|自定义参数
|在代码中引用${key1},${key2}， 然后在“参数”编辑框以如下方式设置key1=value1 key2=value2
|全部任务类型，不支持脚本
|常量参数：param1=”abc” param2=1234；变量参数：param1=$[yyyymmdd], 结果将基于bdp.system.cyctime的取值计算
|自定义参数
|在代码中引用$1 $2 $3， 然后在“参数”编辑框以如下方式设置：value1 value2 value3
|全部任务类型，不支持脚本
|常量参数：”abc” 1234； 变量参数：$[yyyymmdd], 结果将基于bdp.system.cyctime的取值计算
|===

[TIP]
应用场景举例：某任务为每天调度一次，需要统计昨天的历史数据，那么可以在SQL或MR任务中添加业务日期变量，此变量需要随着系统时间而变化，即实现“今天的任务处理昨天的数据”。

[[deve_batch_schedule_param_sys]]
===== 系统参数

DTinsightIDE提供了4个系统参数，定义如下：

* ${bdp.system.cyctime}：一个实例的定时运行时间，默认格式为：yyyyMMddHHmmss。
* ${bdp.system.bizdate}：一个实例计算时对应的业务日期，业务日期默认为定时运行日期的前一天，以yyyyMMdd的格式显示。
* ${bdp.system.currmonth}：一个实例的定时运行时间所在的月份，默认以yyyyMM的格式显示。
* ${bdp.system.premonth}：一个实例的定时运行时间的上一个月，以yyyyMM的格式显示。

从定义可知，运行时间和业务日期有如下计算公式：**运行时间=（业务日期+1）+定时时间**。

若使用系统参数，无需在编辑框设置，直接在代码中引用${bdp.system.bizdate}和${bdp.system.cyctime}即可，系统将自动替换代码中对这两个参数的引用字段。

[NOTE]
一个周期任务的调度属性，配置的是运行时间的定时规律，因此可以根据实例的定时运行时间反推业务日期，从而得知每个实例中这两个参数的取值。


**入门示例**

假设存在一张分区表，以ds作为分区字段，有数据同步任务每天向这张表的一个新的分区写入数据，其分区名由业务日期命名，例如20180412。现需要每天对此表的数据做处理，但只需要处理
当前日期为2018年4月13日，查询此表在20180412分区的数据的SQL为：
[source, SQL]
select * from big_table2 where ds=${bdp.system.bizdate};

此任务运行的日期为2018年4月13日，系统在运行时会将系统参数替换，实际执行的SQL为：

[source, SQL]
select * from big_table2 where ds=20180412;

[NOTE]
DTinsightIDE会将系统参数做简单替换，如果SQL中写为：ds=''${bdp.system.bizdate}''，则也会替换为ds=''20180412''

[[deve_batch_schedule_param_cust]]
===== 自定义变量

使用自定义参数的使用方式：需要先在代码中编辑${key1},${key2}，然后在**任务参数**面板中输入“key1=value1 key2=value2”方可生效。

其中，value的计算分为两种类型：

* 常量：直接替换的字符串或数字，例如“key1=1234 key2=abcdefg”。
* 变量：基于bdp.system.cyctime取值计算出的取值，例如“key1=${yyyy}”表示按bdp.system.cyctime的值取年的部分作为结果替换该参数。关于bdp.system.cyctime的取值，请参见<<deve_batch_schedule_param_sys>>的说明。


*  **变量的格式**

变量的格式支持yyyyMMddHHmmss，其中MM表示月份，mm表示分钟，HH表示24小时制的小时

[NOTE]
默认情况下，自定义变量参数的计算单位为天。例如$[HHmmss-N/24/60]表示(yyyyMMddHHmmss-(N/24/60 * 1天))的计算结果，然后按HHmmss的格式取时分秒。

* **SQL替换示例**
设置一个SQL类型的任务，若在代码中使用一个自定义常量参数tablename和一个自定义变量参数ct，则可按如下步骤进行操作：

在代码中引用自定义变量名。

[source, SQL]
select * from ${tablename} where ds=${ct};

在右侧的**任务参数**面板中自动会出现tablename和ct的输入表单，您直接在表单中输入表名：big_table2和分区名yyyyMMdd，且此任务的定时时间为2018年4月13日，那么系统将其替换为：

[source, SQL]
select * from big_table2 where ds=20180413;


可供参考的变量参数配置方式如下：

* 后N周：yyyyMMdd+7*N
* 前N周：yyyyMMdd-7*N
* 后N天：yyyyMMdd+N
* 前N天：yyyyMMdd-N
* 后N小时：HHmmss+N/24
* 前N小时：HHmmss-N/24
* 后N分钟：HHmmss+N/24/60
* 前N分钟：HHmmss-N/24/60

[[deve_batch_schedule_rely]]
==== 依赖关系

在调度配置中，会需要配置两个任务级别的依赖：任务间依赖和跨周期依赖。

* **任务间依赖**

若某任务B必须在任务A完成后运行，则A为B的上游任务，这种依赖关系可通过如下方式配置：在“上游任务”输入框，输入任务关键字，在列出的可选任务中选中某个任务，此任务被添加到上游任务列表中，即完成了A、B间的依赖关系配置。

[NOTE]
一个任务可以依赖多个上游任务，同样，一个任务可被多个任务依赖。依赖属性为非必填项，当下游任务需依赖上游任务产出数据，则可配置依赖关系。

[NOTE]
上游任务失败后，下游任务不会运行，但其状态会被置为失败状态。

* **跨周期依赖**

配置任务的跨周期依赖，如：天调度任务中，今天需要执行的数据依赖本任务昨天执行的数据，那么可以配置依赖昨天任务的周期，这样一来，昨天的实例必须先执行成功，今天的实例才可以调度起来，这种依赖主要是体现在任务调度实例的依赖。

跨周期依赖配置有两类：

. 自依赖，等待上一调度周期结束，才能继续运行。应用场景：假设设定A任务每天1:00运行，设定为自依赖后，A任务在3月1日未运行成功，则在3月2日不会运行。
. 不依赖上一调度周期：所有任务默认选择该选项，即不依赖任何任务的上周期实例。若A任务在3月1日未运行成功，则在3月2日会产生新的实例并运行。

[NOTE]
依赖属性配置的调度依赖是同周期依赖和跨周期依赖不冲突。任务A可以配置依赖属性依赖任务B，也可以配置跨周期依赖依赖B，如此任务A既依赖任务B，本周期也依赖任务B上周期。

[[deve_batch_script]]
===	脚本开发

脚本文件是对周期任务的补充，通常用于辅助数据开发过程，主要用于实现非周期的临时数据处理，如临时表的增删改等，因此不包含周期属性和依赖关系。

脚本文件仅支持SQL类型，并且仅支持页面直接运行生效，不支持发布，主要使用流程如下图所示：

image::images\05_DTinsightIDEHelp_UserGuide_DataDevelopment-22e7b.png[]

* **新建脚本**

选中左侧的**脚本管理**面板，点击**新建脚本**按钮，填写新建脚本文件弹出框中的配置：

. 脚本名称：需输入英文字母、数字、下划线组成，不超过20个字符。
. 任务类型：目前仅支持SQL类型的脚本，不支持其他类型脚本。
. 存储位置：此任务在页面左侧的任务存储结构中的位置。
. 描述：此任务的描述，可输入长度不超过200个的任意字符。

脚本信息输入完毕后，单击**确认**，脚本文件创建成功。打开创建好的脚本文件，即可进行脚本编辑。


* **编辑脚本**

脚本文件通常用于实现非周期的临时数据处理，如临时表的增删改，一次性的数据初始化或查询任务等。脚本编辑完成后，单击**保存**，下次打开网页时即可看到最近一次保存的内容。

* **运行代码**

保存完毕后，可以选中部分代码单击**运行**。也可以不选中任何代码而直接**运行**，那么将运行全部代码。

* **查看日志**

任务触发运行后，在编辑区下方会显示日志页，如果有语句的运行结果返回了数据集，则在日志页旁显示结果页，也支持结果下载。

无论运行几次，日志页只有一个，仅显示最近一次触发运行的日志信息，之前的日志会被覆盖。结果页可以存在多个，按语句执行顺序依次显示，最多可以显示20个结果页，方便您进行对比数据等操作。

多个语句触发执行时，这些语句将**串行**执行，日志内容依次显示在日志页中。结果则按每个语句的执行顺序分别显示在不同的结果页中。

* **删除脚本**

右键单击选中的脚本，选择**删除**即可。

[[deve_batch_resource]]
=== 资源管理

如果在代码或函数中需要使用.jar等资源文件，那么需要先将资源上传至该项目的项目空间下，然后在函数中进行引用。

[NOTE]
资源管理通常使用在UDF等自定义函数的场景中，因此可以将资源管理理解为函数管理的一个步骤。

image::images\05_DTinsightIDEHelp_UserGuide_DataDevelopment-36a4d.png[]

* **上传资源**

上传资源：可上传jar/Python类型的资源，上传后资源会同步至DTinsightIDE中。

. 资源名称：需输入英文字母、数字、下划线组成，不超过20个字符。
. 资源类型：目前仅支持jar或Python类型的资源，不支持其他类型资源。
. 存储位置：此资源在页面左侧的资源管理存储结构中的位置。
. 描述：此资源的描述，可输入长度不超过200个的任意字符。

DTinsightIDE不支持批量上传资源，同时也请您注意上传资源的大小，超过100M的文件，无法上传。

* **在函数中引用资源**

如果现有的系统内置函数无法满足您的需求，DTinsightIDE支持创建自定义函数，实现个性化处理逻辑。将实现逻辑的Jar包上传至项目空间下，便可在创建自定义函数的时候进行引用。详细操作请参见<<deve_batch_function_udf>>。

* **不支持在代码中引用资源**

DTinsightIDE不支持在代码中引用资源，只支持在函数中引用资源。

* **删除资源**

如果需要删除一个资源，在资源管理中右键单击该资源，选择**删除**即可。

[NOTE]
删除资源后，引用该资源的函数或代码在运行时会报错，故请慎重操作。如有改动，尽量通知到依赖该资源的其他对象的负责人。


[[deve_batch_function]]
=== 函数管理

[[deve_batch_function_apply]]
==== 函数的使用

目前DTinsightIDE的工作对象大部分为SQL类型的脚本和任务。在编辑SQL类型的脚本和任务的代码时，常需要使用各种函数对数据做标准化处理。

函数管理，是DTinsightIDE提供的专用于对SQL 编辑时需要的系统函数和自定义函数进行管理的功能，在此页面可以进行新建目录、新建函数的操作。

函数管理模块下显示的全部函数，无论是系统默认的还是自定义函数，仅用于SQL类型的任务和脚本。

函数的应用场景如下图所示：

image::images\05_DTinsightIDEHelp_UserGuide_DataDevelopment-9df7f.png[]

* **系统函数**

系统默认提供以下几类系统函数，请根据需要，灵活选择系统函数来实现业务需求。

. 日期函数
. 数学函数
. 字符函数
. 聚合函数

* **新建自定义函数**

如果现有的系统函数无法满足需求，DTinsightIDE还支持您创建自定义函数，具体操作请参见<<deve_batch_function_udf>>。

* **查看函数并在代码中引用**

单击函数名，可以查看函数的类型、命令格式以及参数说明。

* **删除函数**

在函数管理页面找到需要删除的函数，右键单击，在菜单栏选择**删除**，即可删除该函数。仅自定义函数可以被删除，系统函数无法被删除。

[[deve_batch_function_udf]]
==== 创建自定义函数

用户自定义函数（User Defined Function，简称 UDF），是用户除了使用 DTinsightIDE提供的内建函数外，自行创建的函数，用于满足个性化的计算需求。自定义函数在使用上与普通的内建函数类似。

本文将通过实现字符小写转换功能的函数，说明用户自定义函数的创建过程，以及如何在DTinsightIDE中使用该函数，具体流程图如下所示：

image::images\05_DTinsightIDEHelp_UserGuide_DataDevelopment-8ed73.png[]

操作步骤：

* **在本地编写代码并编译为Jar包**

在本地Java环境中按照Spark的UDF框架编写Java代码实现函数，本示例的代码如下所示：

[source, java]
import org.apache.hadoop.hive.ql.exec.UDF;
public class HelloUDF extends UDF{
    public String evaluate(String str){
        try{
            return "helloWorld" + str;
        }catch (Exception e){
            return null;
        }
    }
}

将以上代码编译成Jar包。


* **上传资源到DTinsightIDE**

在**任务开发模块**，点击左侧的**资源管理**面板并上传资源文件。在目录树中选择一个文件夹，然后右键选择**上传资源**。

填写资源上传弹出框中的各配置项，提交后资源创建成功。

* **新建自定义函数并引用资源**

进入DTinsightIDE的**数据开发**模块，打开左侧的**函数管理**面板。在目录树中选择一个文件夹，然后右键选择**新建函数**，填写弹出框中的各配置项。

填写完成后，提交后函数创建成功。


* **在SQL任务或脚本中使用函数**

在DTinsightIDE的数据开发页面新建一个SQL任务，创建成功后在代码编辑器中编写SQL语句，如下所示：

[source, SQL]
select evaluate("A")  from dual;

单击**运行**，即可查看结果。

此时，您已经完成自定义函数的创建和使用，并在SQL任务中看到使用效果。

[NOTE]
这里的dual是创建的临时表，您可根据自身需求创建表，建表语句请参见<<deve_batch_develop_table>>。

[[deve_batch_tableview]]
=== 表查询

在**数据开发**页面，表查询模块下，默认会展示当前项目的表。单击表名，即可看到表的列信息、分区信息以及数据预览。

[NOTE]
表查询中不支持新建目录，将表放到目录下分类。

[NOTE]
数据预览的数据是实时的。

[[deve_batch_loadlocaldata]]
=== 导入本地数据

DTinsightIDE支持将保存在本地的文本文件中的数据上传到项目空间的表中。

本地文本文件上传的限制如下：

. 文件类型：仅支持.txt、.csv和.log格式。
. 文件大小：不超过100M。
. 操作对象：导入分区表时，分区不允许为中文。

* **操作步骤**

. 单击**导入**，选择**导入本地数据**。
. 选择本地数据文件，配置导入信息，单击**下一步**。
. 如果导入数据的表已存在，搜索表名即可。
. 如果没有创建导入数据的表，则可以单击**去新建表**，输入建表语句后，单击**确认**。
. 选择导入数据的表名后，选择字段匹配方式（按位置匹配或按名称匹配）。选择按位置匹配以后，如果是分区表，则会提示分区的选择，同时可以点击**检测**按钮，测试分区是否存在，检测后单击**导入**。

[[deve_stream]]
== 实时任务开发

实时任务的开发包括实时任务创建、基于SQL的实时任务开发和基于MR的实时任务开发3大部分。本节首先对实时任务进行简要介绍，并对利用DTinsightIDE进行实时任务开发进行说明。

[[deve_stream_intro]]
=== 实时任务简介

在大数据开发领域，通常根据数据的不同性质，将任务划分为实时计算与离线计算，以温度传感器的场景举例：

例如：某城市安装了大量的温度传感器，每个传感器每隔1min上传一次采集到的温度信息，由气象中心统一汇总，每隔10min更新一次各个地区的温度，这些数据是一直源源不断的产生的，且不会停止。实时计算就主要用于“数据源源不断的产生，而且不会停止，需要以最小的延迟获得计算结果”的场景

[[deve_stream_datainput]]
=== 支持的数据源
在实时任务中，DTinsightIDE目前只支持以**kafka**作为数据源，用户只需要在SQL或MR任务中设定kafka地址、分区等信息即可读取数据进行计算。

**对象**

在实时任务开发阶段，DTinsightIDE提供了3种对象：任务、资源和函数。它们之间的项目关系如下图所示：

image::images\05_DTinsightIDEHelp_UserGuide_DataDevelopment-a4103.png[]

每个对象的说明如下：

* 任务：数据开发的主要对象，是数据计算的主要载体，只支持SQL和MR2种。
* 函数和资源：任务中的代码运行时需要引用的一些文件和计算函数，在任务正式执行前需要上传，使用方式与离线计算类似，详情请参考<<deve_batch>>。


**流程**

一个任务的开发和使用流程如下图所示：

image::images\05_DTinsightIDEHelp_UserGuide_DataDevelopment-6971e.png[]

[[deve_stream_develop]]
===	任务开发

任务开发的过程与离线任务非常类似，下面仅做简要介绍。

[[deve_stream_develop_create]]
==== 新建任务

DTinsightIDE在实时任务处理上提供了2种任务类型，本节以创建SQL任务为例，介绍如何创建一个任务并编辑代码内容。更多任务类型的使用请参见<<deve_batch_type>>。

进入**数据开发**菜单，点击**新建实时任务**按钮，并填写新建任务弹出框中的配置项，配置项说明：

. 任务名称：需输入英文字母、数字、下划线组成，不超过64个字符。
. 任务类型：可选择SQL、MR。
. 资源：在资源管理中上传的Jar包，用于SQL任务的自定义函数或MR任务。
. mainClass（仅用于MR任务）：MR任务中的main类名称。
. 参数（仅用于MR任务）：MR任务中的main类的参数。
. 存储位置：此任务在页面左侧的任务存储结构中的位置。
. 描述：此任务的描述，可输入长度不超过200个的任意字符。

点击“保存”，弹窗关闭，即完成了新建任务，同时系统自动打开新建的SQL任务。

[NOTE]
DTinsightIDE不支持SQL任务对2条或多条流式数据进行JOIN处理，如果需要对流数据进行JOIN处理，需要在本地编写Java代码并上传至DTinsightIDE执行。

[[deve_stream_develop_process]]
==== 实时任务的处理流程

image::images\05_DTinsightIDEHelp_UserGuide_DataDevelopment-8d819.png[]

如上图所示，实时任务的处理流程通常分3步，前2步建立好数据源与数据目标，之后再写数据计算的逻辑，完成对流式数据的分析处理。

[[deve_stream_develop_sql]]
==== SQL任务开发

由于基于Flink作为计算引擎，DTinsightIDE支持对流式数据通过SQL代码进行处理，您只需要在页面中编辑SQL代码即可对流式数据进行清洗和计算。下面进行具体描述：

SQL任务创建好后，可以在代码编辑器中编写SQL语句（该SQL的语法为Flink SQL，与传统关系型数据库的SQL语法有所不同，详细的SQL编辑说明请
https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/table/sql.html[Flink SQL的编码说明]）。

[NOTE]
本版本的DTinsightIDE可以同时支持Flink1.2或Flink1.3版本，具体版本型号请线下咨询。需注意编码规范需符合对应版本的Flink SQL编码规则，版本不一致可能会导致任务失败。

* **添加数据源**

[source, SQL]
CREATE SOURCE TABLE TableName(colName STRING……)
WITH (type='KAFKA09',
bootstrapServers='172.16.1.151:9092',
offsetReset='earliest',
topic='nbTest'
);

参数说明：
[cols="1,3", options="header"]
|===
|参数名
|描述
|TableName
|表名，Flink从kafka中读取的数据将“存储”为此表，用户可以在Flink中访问该表处理数据。TableName后面括号中的colName STRING分别为此表中的字段名和字段类型。
|type
|表示数据源类型，只能填写为“KAFKA09”。DTinsightIDE目前仅支持kafka作为实时计算的数据源。
|bootstrapServers
|访问kafka的host地址和端口号。
|offsetReset
a|
枚举值，可选择earliest或latest。
1、earliest：使用kafka的offset的开始位置。
2、latest：使用kafka offset的最新位置。
|topic
|kafka的topic名称。
|===

* **添加数据目标**

DTinsightIDE实时计算引擎支持通过SQL的形式直接将数据输出到MySQL，只需在SQL中添加如下语句：

[source, SQL]
CREATE RESULT TABLE TableName(colName STRING……)
WITH (type='mysql',
dbURL='jdbc:mysql://172.16.1.203:3306/nb',
userName='dtstack_xc',
password='dtstack_xc',
tableName='pv'
);

[cols="1,3", options="header"]
|===
|参数名
|描述
|TableName
|Flink中的表名，Flink将此表中的数据传输至MySQL数据。
|type
|表示数据源类型，只能填写为“mysql”。DTinsightIDE目前仅支持MySQL作为实时计算的数据输出对象。
|dbURL
|连接此数据库的连接URL。
|userName
|目标数据库的用户名。
|Password
|目标数据库的密码。
|tableName
|目标数据库中的表，Flink将TableName中数据输出至此表。
|===

[NOTE]
本版本的DTinsightIDE仅支持以MySQL作为流式计算的数据输出目标数据库。

[[deve_stream_resource]]
=== 资源管理

如果在代码或函数中需要使用.jar等资源文件，那么需要先将资源上传至该项目的项目空间下，然后在函数中进行引用。

[NOTE]
资源管理通常使用在UDF等自定义函数的场景中，因此可以将资源管理理解为函数管理的一个步骤。

* **上传资源**

实时任务上传资源的操作与离线任务类似，不再赘述。

* **删除资源**

如果需要删除一个资源，在资源管理中右键单击该资源，选择**删除**即可。

[NOTE]
删除资源后，引用该资源的函数或代码在运行时会报错，故请慎重操作。如有改动，尽量通知到依赖该资源的其他对象的负责人。

[[deve_stream_function]]
=== 函数管理

[[deve_stream_function_apply]]
==== 函数的使用

* **系统函数**

系统默认提供以下几类系统函数，请根据需要，灵活选择系统函数来实现业务需求。

. 日期函数
. 数学函数
. 字符函数
. 聚合函数

* **新建自定义函数**

如果现有的系统函数无法满足需求，DTinsightIDE还支持您创建自定义函数，具体操作请参见<<deve_batch_function_udf>>。

* **查看函数并在代码中引用**

单击函数名，可以查看函数的类型、命令格式以及参数说明。

* **删除函数**

在函数管理页面找到需要删除的函数，右键单击，在菜单栏选择**删除**，即可删除该函数。仅自定义函数可以被删除，系统函数无法被删除。

[[deve_stream_function_udf]]
==== 创建自定义函数

用户自定义函数（User Defined Function，简称 UDF），是用户除了使用 DTinsightIDE提供的内建函数外，自行创建的函数，用于满足个性化的计算需求。自定义函数在使用上与普通的内建函数类似。

本文将通过实现字符小写转换功能的函数，说明用户自定义函数的创建过程，以及如何在DTinsightIDE中使用该函数，具体流程图与离线任务类似，不再赘述。下面重点描述UDF在DTinsightIDE的SQL任务中的引用方式：

创建UDF分为2步，首先进行Jar包的加载，之后进行UDF注册。

* **Step1：加载Jar包**

用户需首先使用Eclipse等工具完成UDF逻辑的编写，并通过编译输出Jar包，加载Jar包只需在SQL中添加如下语句：

[source, SQL]
ADD JAR WITH
hdfs://172.16.10.135:9000/rdos/stream/7_clientSource_sofa-1.0-SNAPSHOT.Jar;

参数说明：

. hdfs://hdfs路径，Flink引擎将从指定的路径中加载Jar包。

* **Step2：注册UDF**

完成Jar包加载后，还需要进行UDF的注册，只需在SQL中添加如下语句：

[source, SQL]
CREATE (TABLE|SCALA) FUNCTION functionName WITH
className

参数说明：

. TABLE:TABLE类型UDF，将零个，一个或多个标量值作为输入参数。不同于SCALA UDF，TABLE UDF可以返回任意数量的行作为输出，而不是单个值。返回的行可以由一个或多个列组成。
. SCALA	SCALA类型UDF，将零个，一个或多个标量值映射到一个新的标量值。
. functionName：UDF的函数名。
. className：Jar包中的类名。
