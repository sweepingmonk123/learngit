= 数据集成
//文档整体配置
include::99_documentSetting.adoc[]

数据集成模块是在各个存储单元之间执行数据交换的**通道**。为了在RD-OS进行大规模数据集的挖掘与计算，通常的做法是在任务执行前将数据传输至RD-OS，并在任务执行结束后将计算结果传输至外部存储单元（例如MySQL等应用数据库）。数据集成的作用如下图所示：

.数据输入与输出图
image::images\05_RD-OSHelp_UserGuide_DataIntegration-数据输入与输出.png[]

数据集成模块主要由以下几部分构成：

* 数据源管理
* 数据同步任务的配置与执行

== 支持的数据源

不同的RD-OS版本对数据源的支持能力不同，具体情况请参考<<RD-OS版本信息>>

* {V100image}：支持关系型数据库（MySQL、Oracle、SQLServer）、HDFS
* {V110image}：与V1.0.0相同
* {V120image}：新增HBase数据
* {V160image}：新增Hive数据源


[[dataSourceManagement]]
== 数据源管理

**数据源管理是对外部存储单元访问参数的管理**，数据集成模块需要与<<batchJobSync>>配合起来才能发挥作用，实际是由定时任务来执行数据传输的。

在项目上方的**数据集成**菜单，进入数据数据源管理页面，可看到目前已经集成的数据源列表，包括数据源名称、类型、连接信息、描述、最近修改人、最近修改时间、状态等信息，同时可执行编辑、删除等操作。

* 数据源的配置

** 在项目上方的**数据集成**菜单，进入项目源列表页面；
** 在**数据源列表**右上角的**新增数据源**，选择不同的数据源类型，需要填写不同的配置信息。

. **关系型数据库（MySQL、Oracle、SQLServer）**

*** **JDBC URL**:访问数据库的连接地址，JDBC URL格式如： +
[source, java]
jdbc:mysql://172.16.8.104:3306/test?charset=utf8
*** **用户名**：访问数据库的用户名
*** **密码**：访问数据库的密码

. **Hive**
*** **JDBC URL**:访问Hive的连接地址，JDBC URL格式如： +
[source, java]
jdbc:mysql://172.16.8.104:3306/test?charset=utf8
*** **用户名**：访问数据库的用户名
*** **密码**：访问数据库的密码
*** **DefaultFS**：
*** **高可用配置**：补充高可用配置参数，可以使RD-OS访问高可用模式下的Hive数据源，高可用配置的示例如下：
[source, java]
{
  "dfs.nameservices": "nameserviceTest"
  "dfs.ha.namenodes.nameservice.nameserviceTest":"testDfs",
  "dfs.namenode.rpc-address.nameserviceTest.testDfs":"", "dfs.namenode.rpc-address.nameserviceTest.testDfs":"", "dfs.client.failover.proxy.provider.nameserviceTest": "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"
}

*** 关于HDFS高可用的更多信息可参考
http://hadoop.apache.org/docs/r2.7.4/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html[Hadoop官方文档]


. **HDFS**
*** **DefaultFS**：即HDFS的namenode的节点地址，格式如：`hdfs://ServerIP:Port`。
*** **高可用配置**：补充高可用配置参数，可以使RD-OS访问高可用模式下的HDFS数据源，高可用配置的示例如下：
[source, java]
"hadoopConfig":{
    "dfs.nameservices": "testDfs",
    "dfs.ha.namenodes.testDfs": "namenode1,namenode2",
    "dfs.namenode.rpc-address.testDfs.namenode1": "",
    "dfs.namenode.rpc-address.testDfs.namenode2": "",
    "dfs.client.failover.proxy.provider.testDfs": "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"
}
*** 关于HDFS高可用的更多信息可参考
http://hadoop.apache.org/docs/r2.7.4/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html[Hadoop官方文档]


. **HBase**
*** **Zookeeper集群地址**：
.. 必填，多个地址间用逗号分割。例如：`IP1:Port, IP2:Port, IP3:Port/子目录`。默认是`localhost`,在伪分布式模式时使用。若在完全分布式的情况下使用则需要修改。如果在hbase-env.sh设置了`HBASEMANAGESZK`， 这些ZooKeeper节点就会和HBase一起启动。
.. Port: ZooKeeper的zoo.conf中的配置。客户端连接的端口， 默认2181。
.. 子目录：HBase在ZooKeeper中配置的子目录。
*** **其他参数**：以JSON方式传入其他参数，例如： +
[source, java]
"hbaseConfig": {
  "hbase.rootdir": "hdfs: //ip:9000/hbase",
  "hbase.cluster.distributed": "true",
  "hbase.zookeeper.quorum": "***"
}


. **测试连通性**
*** 在添加/编辑数据源时，可以主动测试数据源的连通性，只有数据源连接正常的情况下才可以被添加/编辑。

* 数据源的状态

已配置的数据源，系统会检测其是否在数据同步任务中被使用，“使用中”状态的数据源只能被编辑，但不能被删除

WARNING: 为了使RD-OS可以访问对应的存储单元（包括读/写），请务必保证RD-OS与这些存储单元之间的网络畅通，否则会导致任务失败。

WARNING: 若您的存储单元配置参数有变更，请及时修改数据源的配置，否则可能导致任务执行失败。

[[batchJobSync]]
== 数据同步任务的配置

数据同步任务的配置共分为5个步骤：

. 选择数据来源：选择已配置的数据源，系统会读取其中的数据；
. 选择数据目标：选择已配置的数据源，系统会向其写入数据；
. 字段映射：配置数据来源与数据目标之间的字段映射关系，不同的数据类型在这步有不同的配置方法；
. 通道控制：控制数据同步的执行速度、错误数据的处理方式等；
. 预览保存：再次确认已配置的规则并保存；

=== 关系型数据库
RD-OS支持MySQL、Oracle、SQLServer3种关系型数据库的数据同步，支持从从这几类数据库中读取/写入数据。

==== 作为数据源

关系型数据库作为数据源，需配置以下信息：

* 选择数据源；
* 选择表；
* 数据过滤条件：针对源头数据筛选条件，根据指定的column、table、where条件拼接SQL进行数据抽取，暂时不支持limit关键字过滤。利用where条件可进行增量同步，具体说明如下：
+
增量导入在实际业务场景中，往往会选择当天的数据进行同步，通常需要编写where条件语句，请先确认表中描述增量字段（时间戳）为哪一个。如tableA增量的字段为create_time，则填写`create_time>您需要的日期`，如果需要日期动态变化，可以填写`${bdp.system.bizdate}`、`${bdp.system.cyctime}`调度参数，关于调度参数的详细信息请参考<<调度参数>>。
* 切分键：RD-OS在进行数据抽取时，如果指定切分键，系统可以使用切分键的字段进行数据分片，数据同步因此会启动并发任务进行数据同步，这样可以大大提供数据同步的效能。
- 推荐将表的主键作为切分键，因为表主键通常情况下比较均匀，因此切分出来的分片也不容易出现数据热点。
- 目前RD-OS目前支持MySQL数据库数值型切分键，支持Oracle数据库数值型、字符串类型切分键，不支持字符串、浮点、日期等其他类型，也不支持其他数据库。如果指定了不支持的类型，则忽略切分键功能，使用单通道进行同步。
- 如果不填写切分键，数据同步视作使用单通道同步该表数据 。
- 切分键的配置需要与第四步的“通道控制”联合使用，下面是典型的应用场景：
+
假设MySQL的tableA表中，有一个INT类型的字段名为id的主键，那么可以在切分键输入框中输入id字段，并在第四步的“作业并发数”中配置为5，则系统会产生5个并发线程同时读取MySQL表，每个任务会根据id字段的值进行数据切分，保证5个并发线程读取的数据是不同的，通过以上机制即可以加速数据读取的速度。

* 映射配置：
- 支持挑选部分字段进行导出。
- 支持字段换序，即字段可以不按照表schema信息顺序进行导出。
//- 支持常量配置，用户需要按照MySQL SQL语法格式，比如 [“id”, “table“, “1”, “‘mingya.wmy’”, “‘null’”, “to_char(a + 1)”, “2.3” , “true”]。 id为普通列名，table为包含保留字的列名，1为整形数字常量，’mingya.wmy’ 为字符串常量（注意需要加上一对单引号），null为空指针，CHAR_LENGTH(s)为计算字符串长度函数，2.3 为浮点数，true为布尔值。

==== 作为数据目标

关系型数据库作为数据目标，需配置以下信息：

* 选择数据同步目标；
* 选择表；
* 导入前、导入后准备语句：执行数据同步任务之前率先执行的SQL语句。目前向导模式只允许执行一条 SQL 语句，例如：清除旧数据。
* 主键冲突的处理：
- replace into模式：**目前只有MySQL数据库支持replace into模式** 没有遇到主键/唯一性索引冲突时，与insert into行为一致，冲突时会用新行替换原有行所有字段；
- insert into模式：当主键/唯一性索引冲突时会写不进去冲突的行，以脏数据的形式体现，脏数据的配置与管理请参考<<错误记录的处理>>；
* 映射配置：支持挑选部分字段进行写入。

=== Hive

==== 作为数据源
* 选择数据源；
* 选择表；
* 分区：读取数据所在的分区信息，支持直接填写分区名称，同时支持系统调度参数:
[source, SQL ]
${bdp.system.premonth}
${bdp.system.cyctime}
${bdp.system.bizdate}
${bdp.system.currmonth}


==== 作为数据目标
* 选择数据源；
* 选择表；
* 分区：需要写入数据表的分区信息，必须指定到最后一级分区。例如把数据写入一个2级分区表，可以通过`pt=20180401/ds=abcdef`。其中pt是一级分区字段，ds是二级分区字段。
- 对于非分区表，该值务必不要填写，表示直接导入到目标表。
- RD-OS不支持数据路由写入，对于分区表请务必保证写入数据到最末一级分区。
- 分区的填写支持直接填写分区名称，同时支持系统调度参数:
[source, SQL ]
${bdp.system.premonth}
${bdp.system.cyctime}
${bdp.system.bizdate}
${bdp.system.currmonth}

* 写入模式：
insert overwrite：写入前将清理已存在的数据文件，之后执行写入；
insert into：写入前已有的数据不会删除，系统会写入的新的文件；

=== HDFS

==== 作为数据源

==== 作为数据目标

=== HBase

==== 作为数据源

==== 作为数据目标

=== 通道控制

==== 同步速率与作业并发

==== 错误记录的处理

=== 版本支持

RD-OS数据集成模块对不同数据库的版本支持情况如下表所示：

[cols="1, 5", options="header"]
|===
|数据源类型
|支持的版本
|MySQL
|5.0及以上
|Oracle
|11g及以上
|SQLServer
|SQLServer 2008及以上
|Hive
|
|HDFS
|
|ElasticSearch
|
|HBase
|
|===

== 最佳实践

=== MySQL导入Hive

=== Hive导入MySQL

=== MySQL数据库增量同步

经<<dataSourceManagement>>的数据源配置后，RD-OS即可以从已配置的数据源中读取或写入数据。下面以MySQL->HDFS，并由RD-OS访问HDFS中的数据为例，介绍操作流程：

=== MySQL导入HDFS

进入<<batchJobSync>>模块，进行如下配置：

.. **数据来源：** 类型选择MySQL，并选中刚刚配置的数据源`dataSourceExample`，选择其中的表`tableAge`，数据过滤规则、切分键不填写。

.. **选择目标：** 类型选择HDFS，并选中刚刚配置的数据源`hdfsSourceExample`，其他配置参数为：

... **路径：**需要填写的格式为`user/hive/warehouse/项目名称.db`。
... **分隔符：**用户自定义，或者不填写，不填写时，会采用HIVE默认的分隔符`\001`。
... **编码：**可选择UTF-8或GBK。
... **文件名：**这里实际上填写的是hive中的表名（本例子中填写为`tableNameAge`），此名称必须与之后的<<createTableAndReadData>>对应起来。
... **文件类型：**支持TEXT和ORC类型，关于这2种文件类型的规则，请参考<<hiveSQLGuide>>。
... **主键冲突规则：**即执行数据同步时，若出现主键冲突，则报错或忽略。

.. **字段映射：** 通过映射的方式，将左右2侧的字段映射起来，用户可通过同名映射或同行映射快速配置。对于HDFS文件，可能出现无法读取字段的问题（RD-OS V1.1.0以及之前的版本存在此问题），用户需手动配置每个HDFS字段。

.. **配置调度周期与依赖关系：**在右侧的任务面板中配置任务的生效日期、调度日期、起调时间、需依赖的上游任务、自依赖等信息，详细配置方式请参考<<batchScheduleConfig>>。

.. **提交任务：**确认任务信息无误后，提交执行，提交成功后，此任务不会立即执行，而是会在当天（或第二天）的22:00生成任务实例，待执行条件满足后才执行，如果需要数据同步任务立即执行，请参考<<jobManagement>>中的**补数据**或<<QA>>。

=== 执行数据同步任务

任务提交成功后，为了立即执行，需在<<jobManagement>>中设置**补数据**任务，即可立即进行数据同步。

=== 新建hive表，并读取数据

在<<dataManagement>>模块中进行如下操作：

. 点击**新建表**，建立**内部表**或**外部表**，内部表与外部表的区别请参考<<QA>>。或点击**DDL建表**，通过SQL的形式建表。
. 新建表的表名，必须与<<syncJobConfig_mysql-hdfs>>中的**文件名**对应，否则会导致无法关联到对应的文件，造成表中没有数据。
. 新建表的编码格式，路径等信息，也必须与<<syncJobConfig_mysql-hdfs>>中的**编码格式**、**路径**对应起来，否则会造成无法关联到对应的文件，造成表中没有数据或编码错误。
. 新建表的字段信息，必须与数据同步任务中的字段对应，否则会造成无法关联到对应的文件。
. 建表成功后，即实现当前表与HDFS数据之间的关联，即可以在<<jobDevelopment>>中当前项目的任意一个SQL查询到数据。

[NOTE]
.新建hive表的注意事项
<<createTableAndReadData>>与<<runSyncJob>>两个步骤不区分先后顺序，但数据同步任务时填写的**路径**、**文件名**、**编码**等参数需要与建hive表时的**路径**、**表名**、**编码**对应起来

NOTE: 在本例中，主键冲突规则的配置生效，是依赖于MySQL中`tableAge`的主键，
